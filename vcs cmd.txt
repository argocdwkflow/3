Voici lâ€™analyse claire et expert Veritas VCS/InfoScale de ce que montrent tes captures.


---

âœ… 1. Ce que montrent les logs Ã  lâ€™Ã©cran

On voit plusieurs messages typiques VCS :

ğŸ”¹ â€œResource became OFFLINE unexpectedlyâ€

Cela signifie :

VCS surveillait la ressource (via le monitor)

Une probe (monitor script) a dÃ©tectÃ© que la ressource nâ€™Ã©tait plus UP

Le parent group peut Ãªtre impactÃ© selon la criticitÃ©


Exemple probable dans ton log :
(Monitor) resource <xxxx> became OFFLINE unexpectedly


---

ğŸ”¹ â€œClearing resource â€¦ completedâ€

Ã‡a veut dire que VCS a effectuÃ© une opÃ©ration de clear automatique aprÃ¨s une fault :

Clear = rÃ©initialiser le fault

Pas un start



---

ğŸ”¹ â€œagent framework (AFP) detected that resource has faultedâ€

Le script dâ€™agent (type Mount, IP, Oracle, NIC, DiskGroup etc.) :

a dÃ©tectÃ© un return code != 110 / 100 / 0

donc a marquÃ© la ressource en FAULTED



---

ğŸ”¹ â€œUnstable resources detected on system XXXâ€

Cela signifie que :

le group ou la ressource nâ€™est pas dans un Ã©tat stable

soit OFFLINE/ONLINE partiel

soit un partial state entre nodes (split-brain VCS non, mais dÃ©salignement)



---

âœ… 2. Le problÃ¨me principal ici

Les logs montrent plusieurs ressources qui tombent OFFLINE sur 1 node, suivies de :

clear automatique

re-probe

mais VCS ne tente pas un start (donc Either)

resources flagged FAULTED


Un node semble ne pas arriver Ã  monitorer les ressources correctement.


---

ğŸ”¥ 3. Causes les plus probables (basÃ© sur logs VCS)

1ï¸âƒ£ Resource offline unexpected â†’ problÃ¨me OS ou FS

Exemples frÃ©quents :

Filesystem mountÃ© mais inaccessible

/etc/vx/.vcsmac ou directory inaccessible

Process disparu

Kernel I/O error (dm-xx, SAN, LUN)

NIC flap â†’ IPMonitor FAIL

Oracle process tuÃ©


ğŸ“Œ Vu que tu as souvent des problÃ¨mes I/O SAN (dm-36, dm-xx) â†’ trÃ¨s probable que VCS dÃ©tecte une probe timeout.


---

2ï¸âƒ£ Lâ€™agent Framework (VxVM, Mount, IP) ne rÃ©pond plus

Typique en cas de :

surcharge CPU

LVM/VxVM â€œhangâ€

systemd en mode freeze (tu en as parlÃ© rÃ©cemment)

unhang kernel threads (D-state)



---

3ï¸âƒ£ Heartbeat VCS OK â†’ pas un split brain

Dâ€™aprÃ¨s le log, pas de :

LLT WARNING GAB membership lost

Donc cluster OK.


---

ğŸ¯ 4. Les commandes Ã  exÃ©cuter pour analyser (trÃ¨s important)

1. Voir lâ€™Ã©tat cluster

hastatus -sum

2. Voir ressources en fault

hares -state | grep -i FAULT

3. Voir les lastprobes

hastatus -display | grep -i last

4. Logs dÃ©taillÃ©s

/var/VRTSvcs/log/engine_A.log
/var/VRTSvcs/log/A.log
/var/VRTSvcs/log/agentname_A.log

5. VÃ©rifier la ressource elle-mÃªme

Selon le type :

hagrp -display <group>
hares -display <resource>


---

ğŸ§  5. Lâ€™analyse la plus probable

ğŸ‘‰ Le node a des problÃ¨mes dâ€™I/O ou filesystem, causant :

monitor timeout

resource probe error

resource â†’ FAULTED

VCS ne redÃ©marre pas car â€œunexpected offlineâ€ (par design)

VCS clear la ressource mais elle retombe


Cela concorde avec tes problÃ¨mes rÃ©currents de :

/var/cache/yum

dm-36 / SAN

local-fs.target fail

I/O error buffer

LVM freeze


ğŸ’¡ VCS nâ€™est pas le problÃ¨me â€“ câ€™est le symptÃ´me !

Le problÃ¨me est probablement OS / I/O / kernel / FS.


---

ğŸš€ 6. Ce que tu dois faire maintenant (en tant quâ€™expert N3)

Ã‰tape 1 : VÃ©rifier si les FS des ressources sont accessibles

ls -l /apps /oracle /var/...

Ã‰tape 2 : VÃ©rifier les erreurs I/O ou kernel

dmesg | grep -i error
dmesg | grep -i dm-
journalctl -xe | grep -i xfs

Ã‰tape 3 : VÃ©rifier si un agent est bloquÃ©

ps -ef | grep -i hatype

Ã‰tape 4 : Clear manual

hares -clear <resource> -sys <node>

Ã‰tape 5 : If needed : switch node

hagrp -switch <group> -to <othernode>


---

ğŸ 7. Si tu veux que je fasse un diagnostic complet

Envoie-moi :

âœ” sortie de hastatus -sum
âœ” sortie de hares -state
âœ” sortie de dmesg | tail -200
âœ” engine_A.log (les 30 derniÃ¨res lignes)

Je tâ€™analyse tout comme un expert Veritas InfoScale N3 et je te donne la root cause exact.


---

Si tu veux, je peux aussi te produire :

ğŸ”¥ un RCA complet

ğŸ”¥ un email propre pour lâ€™Ã©quipe VCS / stockage / OS

ğŸ”¥ un plan de remÃ©diation cluster


Souhaites-tu une analyse RCA complÃ¨te Ã  envoyer Ã  ton management / Ã©quipe ?


Parce que, pour VCS, â€œFAULTEDâ€ = la ressource devait Ãªtre ONLINE mais le monitor lâ€™a trouvÃ©e OFFLINE (ou en erreur) de faÃ§on inattendue.

Sur ton screenshot on lit (en le remettant Ã  lâ€™endroit) quelque chose du genre :

Resource mfes_app is faulted on system s02r1v9â€¦

agent framework (AFP) because the resource became OFFLINE unexpectedly on its own

VCS will not initiate failover of service group SG_PROD_MF_ROOM because the faulted resource is not critical and no critical parent resources need to be taken offline


Donc la sÃ©quence est :

1. Le service group SG_PROD_MF_ROOM pensait que mfes_app Ã©tait ONLINE sur le node â€¦13.


2. Le script de monitor de lâ€™agent (type Application, Mount, etc.) a fait un check.


3. Ce check a retournÃ© une erreur (process disparu, retour â‰  0, timeout, filesystem inaccessible, etc.).


4. VCS marque alors la ressource FAULTED sur ce node et Ã©crit â€œbecame OFFLINE unexpectedlyâ€.


5. Comme mfes_app nâ€™est pas marquÃ©e Critical, VCS nâ€™initie pas de failover du SG complet (câ€™est ce quâ€™explique la phrase sur â€œnot critical and no critical parent resourcesâ€).



ğŸ‘‰ Autrement dit :
mfes_app a Ã©tÃ© arrÃªtÃ©e / a crashÃ© / est devenue inaccessible au niveau OS sur le node xxxx13, alors que VCS la considÃ©rait ONLINE.
Ce nâ€™est pas VCS qui lâ€™a arrÃªtÃ©e, VCS ne fait que constater lâ€™Ã©tat et la marquer en FAULTED.


---

Ce que tu peux vÃ©rifier concrÃ¨tement sur le node xxxx13

1. Type et config de la ressource :



hares -display mfes_app

(regarde Type, MonitorProgram, MonitorProcesses, Critical, RestartLimitâ€¦)

2. Son Ã©tat prÃ©cis :



hares -state mfes_app -sys xxxx13
hastatus -sum | egrep "GROUP|mfes_app"

3. CÃ´tÃ© OS au moment du fault (mÃªme horodatage que dans le log VCS) :



dmesg | tail -200
journalctl -xe --since "YYYY-MM-DD HH:MM:SS" --until "YYYY-MM-DD HH:MM:SS"
ps -ef | grep -i mfes

Tu cherches :

crash du process mfes_*

kill manuel (SIGKILL)

erreurs I/O sur les LUN (dm-xx), XFS, etc.
(trÃ¨s probable vu tes buffer I/O error / dm-36 sur ce cluster)


4. AprÃ¨s correction de la cause OS / appli :



hares -clear mfes_app -sys xxxx13
hagrp -online SG_PROD_MF_ROOM -sys xxxx13   # si tu veux le remettre ici

Si tu mâ€™envoies :

hares -display mfes_app

hastatus -sum

les logs engine_A.log autour de cet Ã©vÃ©nement
je peux te faire une petite RCA complÃ¨te ciblÃ©e pour ton Ã©quipe (OS / stockage / Veritas).


Voici la mÃ©thode correcte, propre et 100% â€œVCS best-practiceâ€ pour arrÃªter et dÃ©marrer la ressource mfes_APP afin de tester son comportement sans impacter le cluster, de maniÃ¨re isolÃ©e et contrÃ´lÃ©e.


---

âœ… 1. Comprendre avant dâ€™agir

mfes_APP est une resource appartenant Ã  un service group (SG) du cluster VCS.
â¡ï¸ Tu NE dois jamais utiliser systemctl ou tuer les process manuellement
â¡ï¸ Tu dois toujours utiliser les commandes VCS (hares / hagrp)

Sinon :

VCS croira que la ressource â€œdevient OFFLINE unexpectedlyâ€

Elle passera en FAULTED

Le Service Group peut partir en failover selon les attributs Critical et AutoFailover



---

âœ… 2. Trouver le Service Group de la ressource

Commande simple :

hagrp -where mfes_APP

ou

hares -display mfes_APP | grep -i Group

Exemple rÃ©sultat :

Group = SG_PROD_MF_ROOM


---

âœ… 3. ArrÃªter proprement la ressource (VCS contrÃ´le lâ€™arrÃªt)

MÃ©thode officielle :

hares -offline mfes_APP -sys <node>

Exemple :

hares -offline mfes_APP -sys s02r1v9xxx13

ğŸ’¡ Ceci appelle le script de lâ€™agent :

StopProgram ou

Stop entry point
Il arrÃªte proprement sans mettre la ressource en FAULTED.



---

ğŸ“Œ VÃ©rifier lâ€™Ã©tat aprÃ¨s lâ€™arrÃªt

hares -state mfes_APP -sys <node>

RÃ©sultat attendu :

OFFLINE


---

âœ… 4. DÃ©marrer proprement la ressource

hares -online mfes_APP -sys <node>

ou (Ã©quivalent)

hagrp -online <Group> -sys <node>

Exemple :

hagrp -online SG_PROD_MF_ROOM -sys s02r1v9xxx13


---

ğŸ“Œ VÃ©rifier que le monitor fonctionne

hares -state mfes_APP -sys <node>
hastatus -sum | grep mfes_APP

La ressource doit passer :

ONLINE|ONLINE


---

ğŸ” 5. Tester le monitor (health check VCS)

Pour valider le fonctionnement monitoring sans arrÃªter lâ€™application, tu peux forcer un monitor :

hares -probe mfes_APP -sys <node>

Puis vÃ©rifier :

hares -state mfes_APP


---

âš ï¸ 6. Si la ressource passe FAULTED pendant tes tests

Clear :

hares -clear mfes_APP -sys <node>

Puis relancer :

hares -online mfes_APP -sys <node>


---

ğŸ§  7. RÃ©sumÃ© des commandes (Ã  garder dans ton runbook)

â–¶ï¸ Trouver le SG liÃ© :

hagrp -where mfes_APP

â–¶ï¸ ArrÃªter la ressource :

hares -offline mfes_APP -sys <node>

â–¶ï¸ DÃ©marrer la ressource :

hares -online mfes_APP -sys <node>

â–¶ï¸ Prober (monitor test) :

hares -probe mfes_APP -sys <node>

â–¶ï¸ Clear en cas de FAULTED :

hares -clear mfes_APP -sys <node>

â–¶ï¸ VÃ©rifier :

hares -state mfes_APP -sys <node>
hastatus -sum


---

ğŸ”¥ Si tu veux, je peux aussi te prÃ©parer :

un runbook PDF "VCS Resource Operations"

un script vcs_resource_test.sh

une procÃ©dure complÃ¨te â€œFailover test MFES_APPâ€


Veux-tu que je gÃ©nÃ¨re le runbook ?
